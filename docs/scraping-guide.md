# üï∑Ô∏è Gu√≠a de Scraping con GitHub Actions

Esta gu√≠a explica c√≥mo ejecutar los scrapers de Smashly en la nube usando GitHub Actions, sin necesidad de tener tu ordenador encendido.

---

## üìã √çndice

1. [¬øPor qu√© GitHub Actions?](#por-qu√©-github-actions)
2. [Configuraci√≥n Inicial](#configuraci√≥n-inicial)
3. [Ejecutar los Scrapers](#ejecutar-los-scrapers)
4. [Descargar los Resultados](#descargar-los-resultados)
5. [Workflows Disponibles](#workflows-disponibles)
6. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## ü§î ¬øPor qu√© GitHub Actions?

**Ventajas:**

- ‚úÖ **Gratis** para repositorios p√∫blicos (2000 min/mes)
- ‚úÖ **No requiere servidor propio** ni infraestructura
- ‚úÖ **Ejecuta en la nube** - no necesitas tu ordenador encendido
- ‚úÖ **6 horas de timeout** por job (suficiente para ~1000 productos)
- ‚úÖ **Resultados autom√°ticos** - se guardan como artifacts o commits

**Limitaciones:**

- ‚ö†Ô∏è M√°ximo 6 horas por job (si se pasa, usa el workflow en modo "split")
- ‚ö†Ô∏è Recursos limitados (2 CPU cores, 7GB RAM)

---

## ‚öôÔ∏è Configuraci√≥n Inicial

### 1. Verificar que el repositorio est√© en GitHub

```bash
git remote -v
# Debe aparecer: origin  https://github.com/codeurjc-students/2025-Smashlyapp.git
```

### 2. Hacer push de los workflows

```bash
git add .github/workflows/run-scrapers.yml
git add .github/workflows/run-scrapers-split.yml
git commit -m "feat: add GitHub Actions workflows for scrapers"
git push origin main
```

### 3. Verificar que los workflows est√©n activos

1. Ve a tu repositorio en GitHub
2. Click en la pesta√±a **"Actions"**
3. Deber√≠as ver:
   - `Run Padel Scrapers` (modo normal)
   - `Run Padel Scrapers (Split Mode)` (modo paralelo)

---

## üöÄ Ejecutar los Scrapers

### Opci√≥n 1: Modo Normal (recomendado para la primera vez)

**√ösalo cuando:**

- Es tu primera ejecuci√≥n y no sabes cu√°nto tarda
- Quieres scrapear todas las tiendas a la vez
- Estimas que tardar√° menos de 6 horas

**Pasos:**

1. Ve a **GitHub ‚Üí Actions ‚Üí Run Padel Scrapers**
2. Click en **"Run workflow"** (bot√≥n gris a la derecha)
3. Configura los par√°metros:
   - **stores**: `all` (o espec√≠ficas: `padelmarket,padelnuestro`)
   - **limit**: Vac√≠o para todo, o un n√∫mero (ej: `50` para probar)
4. Click en **"Run workflow"** (bot√≥n verde)

**Monitorear el progreso:**

- Ver√°s el job ejecut√°ndose en tiempo real
- Los logs muestran cada producto scraped
- Puedes cancelarlo en cualquier momento si es necesario

---

### Opci√≥n 2: Modo Split (si supera 6 horas)

**√ösalo cuando:**

- El modo normal se qued√≥ sin tiempo (timeout)
- Quieres aprovechar paralelizaci√≥n (m√°s r√°pido)
- Cada tienda se ejecuta independientemente

**Pasos:**

1. Ve a **GitHub ‚Üí Actions ‚Üí Run Padel Scrapers (Split Mode)**
2. Click en **"Run workflow"**
3. Configura **limit** si quieres (opcional)
4. Click en **"Run workflow"**

**Ventajas:**

- Cada tienda tiene **6 horas propias** (total: 24h disponibles)
- Se ejecutan **en paralelo** (termina m√°s r√°pido)
- Al final se **fusionan autom√°ticamente** los resultados

---

## üì• Descargar los Resultados

### M√©todo 1: Artifacts (descarga manual)

1. Cuando el workflow termine, ve a la ejecuci√≥n
2. Scroll hasta abajo ‚Üí **"Artifacts"**
3. Descarga `rackets-data-XXX.zip`
4. Descomprime ‚Üí Dentro est√° `rackets.json`

**Luego copia el archivo a tu proyecto local:**

```bash
# Reemplaza tu rackets.json local con el descargado
cp ~/Downloads/rackets.json /Users/teijeiro7/Documents/Proyectos/2025-Smashlyapp/src/scrapers/rackets.json
```

---

### M√©todo 2: Commit Autom√°tico (recomendado)

El workflow **autom√°ticamente** hace commit del `rackets.json` al repositorio.

**Para obtenerlo:**

```bash
git pull origin main
```

¬°Listo! El archivo `src/scrapers/rackets.json` se actualiza autom√°ticamente.

---

## üìä Workflows Disponibles

### 1. `run-scrapers.yml` - Modo Normal

**Ejecuta las tiendas secuencialmente (una tras otra)**

| Par√°metro | Descripci√≥n          | Ejemplo                            |
| --------- | -------------------- | ---------------------------------- |
| `stores`  | Tiendas a scrapear   | `all` o `padelmarket,padelnuestro` |
| `limit`   | Productos por tienda | Vac√≠o (sin l√≠mite) o `50`          |

**Timeout:** 6 horas total

---

### 2. `run-scrapers-split.yml` - Modo Paralelo

**Ejecuta cada tienda en un job separado (paralelamente)**

| Par√°metro | Descripci√≥n          | Ejemplo                    |
| --------- | -------------------- | -------------------------- |
| `limit`   | Productos por tienda | Vac√≠o (sin l√≠mite) o `100` |

**Timeout:** 6 horas **por tienda** (24h totales disponibles)

**Jobs:**

- `scrape-padelmarket`
- `scrape-padelnuestro`
- `scrape-padelproshop`
- `scrape-tiendapadelpoint`
- `merge-results` (fusiona todo al final)

---

## üõ†Ô∏è Soluci√≥n de Problemas

### ‚ùå Error: "Timeout despu√©s de 6 horas"

**Soluci√≥n:** Usa el workflow `run-scrapers-split.yml`

```bash
# Cada tienda tendr√° 6h propias
```

---

### ‚ùå Error: "playwright not found"

**Causa:** A veces Playwright no se instala correctamente

**Soluci√≥n:** El workflow ya incluye el paso `playwright install-deps`. Si falla, revisa los logs del step "Install Playwright browsers".

---

### ‚ùå Error: "Permission denied" al hacer commit

**Causa:** El token de GitHub no tiene permisos

**Soluci√≥n:** Ve a **Settings ‚Üí Actions ‚Üí General ‚Üí Workflow permissions** y activa:

- ‚úÖ Read and write permissions

---

### ‚ö†Ô∏è Los resultados est√°n duplicados

**Causa:** Ejecutaste ambos workflows a la vez

**Soluci√≥n:** Usa solo uno de los workflows. Si tienes `rackets.json` duplicado, ejecuta:

```bash
cd src/scrapers
python clean_rackets.py  # Si tienes un script de limpieza
```

---

### üêõ Ver logs detallados

1. Ve a la ejecuci√≥n del workflow
2. Click en el job (ej: `scrape`)
3. Expande cada step para ver los logs
4. Busca mensajes de error espec√≠ficos

---

## üí° Consejos √ötiles

### Primera Ejecuci√≥n - Prueba con L√≠mite

```yaml
stores: all
limit: 10 # Solo 10 productos por tienda para probar
```

Esto te permite:

- ‚úÖ Verificar que todo funciona
- ‚úÖ Estimar cu√°nto tarda (10 productos √ó 4 tiendas = 40 productos)
- ‚úÖ Calcular tiempo total: si 40 productos tardan X minutos, 1000 tardar√°n ~25X minutos

---

### C√°lculo de Tiempo Estimado

**F√≥rmula aproximada:**

```
Tiempo por producto ‚âà 5-10 segundos (con Playwright)
1000 productos √ó 8 seg ‚âà 8000 seg ‚âà 2.2 horas
```

**‚ö†Ô∏è Var√≠a seg√∫n:**

- Velocidad de respuesta de las tiendas
- Tama√±o de las p√°ginas
- Cantidad de im√°genes
- Runners de GitHub (pueden ser m√°s lentos que tu local)

---

### Ejecutar Solo una Tienda Espec√≠fica

```yaml
stores: padelmarket
limit: (vac√≠o)
```

√ötil para:

- Probar scrapers individuales
- Re-scrapear una tienda que fall√≥
- Debugging

---

## üìÖ Automatizaci√≥n Programada (Opcional)

Si quieres ejecutar los scrapers **autom√°ticamente** cada cierto tiempo:

**Edita el workflow y a√±ade:**

```yaml
on:
  workflow_dispatch: # Mantener la ejecuci√≥n manual
  schedule:
    - cron: "0 2 * * 0" # Cada domingo a las 2 AM UTC
```

**Ejemplos de cron:**

- `0 2 * * 0` - Cada domingo a las 2 AM
- `0 3 * * 1` - Cada lunes a las 3 AM
- `0 4 1 * *` - El d√≠a 1 de cada mes a las 4 AM

---

## üéØ Recomendaci√≥n Final

**Para tu caso (1000 productos, ejecuci√≥n √∫nica):**

1. **Primero:** Ejecuta `run-scrapers.yml` con `limit: 20` ‚Üí Ver cu√°nto tarda
2. **Calcular:** Si 20 productos tardan X minutos, 1000 tardar√°n ~50X minutos
3. **Si <6h:** Ejecuta `run-scrapers.yml` sin l√≠mite
4. **Si >6h:** Ejecuta `run-scrapers-split.yml` sin l√≠mite

**Ventaja del modo split:**

- Aunque tarde m√°s de 6h en total, cada tienda tiene su propio l√≠mite
- Se ejecutan en paralelo ‚Üí Termina en ~1/4 del tiempo

---

## üìû Ayuda

Si algo falla, revisa:

1. Los **logs del workflow** en GitHub Actions
2. El archivo `scraper.log` en los artifacts descargados
3. Que las dependencias est√©n en `requirements.txt`

---

**¬°Listo para scrapear sin preocuparte de tener el ordenador encendido! üöÄ**
